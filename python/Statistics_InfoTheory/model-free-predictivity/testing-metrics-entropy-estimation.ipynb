{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric - Transfer Entropy\n",
    "\n",
    "### Statistical preliminaries:\n",
    "* $H(X) = -\\sum_i p_i \\log p_i$\n",
    "  - Entropy: the average number of bits of information obtained from a measurement of X\n",
    "* $H(X|Y) = H(XY) - H(X)$\n",
    "  - Conditional Entropy: the information carried by X if Y is already known\n",
    "* $I(X;Y) = H(X) - H(X|Y) = H(X) + H(Y) - H(XY)$\n",
    "  - Mutual information: the information carried by both X and Y at the same time\n",
    "* $I(X;Y|Z) = I(X;YZ) - I(X;Z)$\n",
    "  - Conditional mutual information: the mutual information between X and Y, given that Z is known\n",
    "\n",
    "### Empirical estimates\n",
    "\n",
    "When constructing an empirical estimate for entropy, one runs into the following problems:\n",
    "* For an arbitrary discrete function, an empirical estimate of the entropy converges only if the number of samples is larger than the size of the support. For an arbitrary continuous function, an empirical estimate will always be wrong.\n",
    "* Typical solution is to destroy information, discretizing possible variable values to a few bins, and thus reducing the function to a finite support. While it is a valid procedure to compare probability distributions discretized in the same way, the result depends strongly on the size of the bins.\n",
    "\n",
    "Given the binned empirical probability distribution $\\hat{\\vec{p}}$, the naive MLE estimate is given by\n",
    "\n",
    "$\\hat{H}_{MLE}(\\hat{\\vec{p}}) = -\\sum_i \\hat{p}_i \\log \\hat{p}_i$\n",
    "\n",
    "According to <a href=\"https://kaushikghose.wordpress.com/2013/10/24/computing-mutual-information-and-other-scary-things/\">this blog</a>, the naive MLE of entropy has a large bias. It is suggested to use JackKnife bias-corrected estimator instead. It is still biased, but much less, and also less sensitive to bin size\n",
    "\n",
    "$\\hat{H}_{JK}(\\hat{\\vec{p}}) = N\\hat{H}_{MLE}(\\hat{\\vec{p}})\n",
    "  - \\frac{N-1}{N}\\sum_{i=1}^N \\hat{H}_{MLE}(\\hat{\\vec{p}} // \\hat{p}_i)\n",
    "$\n",
    "\n",
    "where the second term estimates entropy with one less bin for all bins. Further info:\n",
    "* <a href=\"https://mitpress.mit.edu/books/spikes\">Bialek1997</a> discuss estimating entropy using a prior on smoothness of the underlying distribution. They conclude that the estimator is highly sensitive to smoothness assumptions, which may be hard to obtain for neural data.\n",
    "* <a href=\"http://www.cns.nyu.edu/pub/lcv/paninski-infoEst-2003.pdf\"> Paninski2003 </a> show that any estimator is strongly biased if $N_{data} \\leq N_{bin}$. They also derive an even better approximation called \"Best Upper Bounds\". It is highly involved though. \n",
    "* <a href=\"https://journals.aps.org/pre/abstract/10.1103/PhysRevE.71.066208\">Celucci2005</a> offer another improvement to the algorithm by introducing adaptive binning to best represent the information contained in the data.\n",
    "\n",
    "**TODO**: Ask Liam for $H_{BUB}$ code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npExclude(arr, i):\n",
    "    return np.hstack(arr[:i], arr[i+1:])\n",
    "\n",
    "def H_X(PX):\n",
    "    return np.sum([p * np.log2(1.0 / p) for p in PX[:, 1] if p > 0])\n",
    "\n",
    "def H_X_JK(PX):\n",
    "    N = len(PX)\n",
    "    return N*H_X(PX) - (N-1)/N * np.sum([H_X(npExclude(PX, i)) for i in range(N)])\n",
    "\n",
    "def H_XY(PXY):\n",
    "    return H_X(PXY[1:])\n",
    "\n",
    "def HC_XY(PXY, PY):\n",
    "    return H_XY(PXY) - H_X(PX)\n",
    "\n",
    "def I_XY(PXY, PX, PY):\n",
    "    return H_X(PX) + H_X(PY) - H_XY(PXY)\n",
    "\n",
    "# def IC_XYZ():\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlated_x(N, rho=0.5):\n",
    "    r = np.random.randn(N, 2)\n",
    "    r1 = r[:,0]\n",
    "    r1_ = r[:,1]\n",
    "    r2 = rho*r1 + (1-rho**2)**0.5*r1_\n",
    "    return r1, r2\n",
    " \n",
    "def bin_data(x,y, bins=11, limits=[-4,4]):\n",
    "    Nxy, xe, ye = np.histogram2d(x,y,bins=bins,range=[limits,limits])\n",
    "    N = float(Nxy.sum())\n",
    "    Pxy = Nxy/N\n",
    "    Px = Pxy.sum(axis=1)\n",
    "    Py = Pxy.sum(axis=0)\n",
    "    return Pxy, Px, Py, N\n",
    " \n",
    "MI_exact = lambda rho: -.5*np.log2(1-rho**2)\n",
    " \n",
    "def H_mle(P):\n",
    "    idx = pylab.find(P>0)\n",
    "    return -(P.flat[idx]*np.log2(P.flat[idx])).sum()\n",
    " \n",
    "def MI_mle(x,y, bins=11, limits=[-4,4]):\n",
    "    Pxy, Px, Py, Ntot = bin_data(x,y,bins=bins,limits=limits)\n",
    "    return H_mle(Px) + H_mle(Py) - H_mle(Pxy)\n",
    " \n",
    "def MI_mle_jack_knife(x, y, bins=11, limits=[-4,4]):\n",
    "    Pxy, Px, Py, N = bin_data(x,y,bins=bins,limits=limits)\n",
    "    Hx = H_mle(Px)\n",
    "    Hy = H_mle(Py)\n",
    "    Hxy = H_mle(Pxy)\n",
    "    Hx_jk = 0\n",
    "    Hy_jk = 0\n",
    "    Hxy_jk = 0\n",
    "    for n in range(x.size):\n",
    "        jx = np.concatenate((x[:n],x[n+1:]))\n",
    "        jy = np.concatenate((y[:n],y[n+1:]))\n",
    "        Pxy, Px, Py, Njk = bin_data(jx,jy,bins=bins,limits=limits)\n",
    "        Hx_jk += H_mle(Px)\n",
    "        Hy_jk += H_mle(Py)\n",
    "        Hxy_jk += H_mle(Pxy)\n",
    " \n",
    "    Hx_jk = N*Hx - (N-1.0)/N*Hx_jk\n",
    "    Hy_jk = N*Hy - (N-1.0)/N*Hy_jk\n",
    "    Hxy_jk = N*Hxy - (N-1.0)/N*Hxy_jk\n",
    " \n",
    "    return Hx_jk + Hy_jk - Hxy_jk\n",
    " \n",
    "def runmany(func, N=50, rho=0, bins=11, limits=[-4,4], b=1000):\n",
    "    mi = np.empty(b)\n",
    "    for n in range(b):\n",
    "        r1,r2 = correlated_x(N, rho)\n",
    "        mi[n] = func(r1,r2,bins=bins)\n",
    "    mi.sort()\n",
    "    med_idx = int(b*0.5)\n",
    "    lo_idx = int(b*0.025)\n",
    "    hi_idx = int(b*0.975)\n",
    "    return mi[lo_idx], mi[med_idx], mi[hi_idx]\n",
    " \n",
    "b = 1000\n",
    "Ni = 20\n",
    "rho = np.linspace(0,.99,Ni)\n",
    "mi_exact = MI_exact(rho)\n",
    "N = 100\n",
    "cols = [(.5,.5,.5), (.7,.7,.7)]\n",
    "for lab, func in zip(['MI MLE', 'MI Jack Knife'], [MI_mle, MI_mle_jack_knife]):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for k, bins in enumerate([11,21]):\n",
    "        mi_est = np.empty((Ni,3))\n",
    "        for n in range(rho.size):\n",
    "            #mi_est[n,:] = runmany(MI_mle_jack_knife, N=N, rho=rho[n], bins=bins, limits=[-4,4], b=b)\n",
    "            #mi_est[n,:] = runmany(MI_mle, N=N, rho=rho[n], bins=bins, limits=[-4,4], b=b)\n",
    "            mi_est[n,:] = runmany(func, N=N, rho=rho[n], bins=bins, limits=[-4,4], b=b)\n",
    "        plt.fill_between(rho, mi_est[:,0], mi_est[:,2],color=cols[k],edgecolor='k',alpha=.5)\n",
    "        plt.plot(rho, mi_est[:,1], 'k', lw=2)\n",
    "        plt.plot(rho, mi_exact, 'b',lw=2)\n",
    "        plt.xlabel(r'$\\rho$')\n",
    "        plt.ylabel(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction into phase-space metrics\n",
    "# Below code demonstrates that entropy is related to the area of the support\n",
    "NDATA = 500\n",
    "NBINS = 100\n",
    "\n",
    "data = np.random.randint(0, NBINS, NDATA)\n",
    "bins = np.zeros(NBINS)\n",
    "nzero = np.zeros(NDATA)\n",
    "entropy = np.zeros(NDATA)\n",
    "\n",
    "for i in range(NDATA):\n",
    "    bins[data[i]] += 1\n",
    "    nzero[i] = np.count_nonzero(bins)\n",
    "    distr = bins / (i+1)\n",
    "    entropy[i] = H_X(np.array([(0, p) for p in distr]))\n",
    "    \n",
    "def exp_ndata(n, nbin):\n",
    "    return 1 - (1 - 1/nbin)**n\n",
    "    \n",
    "expected = exp_ndata(np.linspace(1, NDATA+1, NDATA), NBINS)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(entropy / np.log2(NBINS), label='entropy')\n",
    "plt.plot(expected, label='area-ratio-analytic')\n",
    "plt.plot(nzero / NBINS, label='area-ratio-real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
