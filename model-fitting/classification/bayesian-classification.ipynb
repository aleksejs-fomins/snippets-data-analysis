{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Statistics\n",
    "\n",
    "$$P[D|\\theta] = \\frac{P[\\theta|D]P[D]}{P[\\theta]} = \\frac{P[\\theta|D]P[D]}{\\int_D P[\\theta|D]P[D]dD}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayesian Classifier\n",
    "\n",
    "Let\n",
    "* $x$ be a multivariate observable\n",
    "* $y$ be a categorical variable denoting the class from which $x$ was drawn\n",
    "* $P[x | y, \\theta]$ be the known forwards model for each class (aka likelihood)\n",
    "* For multiple observations, the likelihood is $P[\\vec{x} | \\vec{y}, \\theta]$\n",
    "\n",
    "Using Bayes rule, we arrive at posterior\n",
    "\n",
    "$$P[\\vec{y} | \\vec{x}] = \\frac{1}{Z(\\vec{x}, \\theta)} P[\\vec{x} | \\vec{y}, \\theta] P[\\vec{y}]$$\n",
    "\n",
    "where $P[\\vec{y}]$ is the prior related to observing a set of labels in that sequence, and $Z = \\sum_y P[\\vec{x} | y, \\theta] P[y]$ is the model evidence.\n",
    "\n",
    "* Naive approximation assumes that all observables are i.i.d, that is, unpredictable from one another. Then\n",
    "\n",
    "$$P[\\vec{x} | \\vec{y}, \\theta] = \\prod_i P[x_i | y_i, \\theta]$$\n",
    "\n",
    "$$P[\\vec{y}] = \\prod_i P[y_i]$$\n",
    "\n",
    "* Construct negative log-likelihood for the posterior\n",
    "\n",
    "$$\\lambda(x, y, \\theta) = -2\\log P[\\vec{y} | \\vec{x}] = -2\\sum_i \\log P[x_i | y_i] - 2\\sum_i \\log P[y_i] + 2\\log Z(\\vec{x}, \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flavours\n",
    "\n",
    "## ML approach\n",
    "\n",
    "Find the MAP estimator for the training set\n",
    "\n",
    "$$\\hat \\theta_{MAP} = \\mathrm{argmin}_\\theta \\; \\lambda(x, y, \\theta)$$\n",
    "\n",
    "Then perform classification using posterior probabilities\n",
    "\n",
    "$$\\hat{y}_{test} = \\mathrm{argmax}_y \\; P[y| x_{test}, \\hat \\theta_{MAP}]$$\n",
    "\n",
    "## Bayesian Naive Bayes\n",
    "\n",
    "Instead of searching for optimal model parameters $\\theta$ to perform classification, priors are used to calculate expected marginal probability over all possible parameters. If the integral happens to be analytically tractable, it will only be a function of the training data and prior parameters. Below $x,y$ will refer to the test set variables, and $D$ will refer to the training set\n",
    "\n",
    "$$P[y|x, D] \\sim P[y, x, D]\n",
    "= \\int_\\theta P[y, x, D | \\theta]P[\\theta]d\\theta\n",
    "= \\int_\\theta P[y, x|\\theta] \\prod_i P[x^{data}_i, y^{data}_i | \\theta]P[\\theta]d\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
