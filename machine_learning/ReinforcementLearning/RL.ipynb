{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning\n",
    "OCNC 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes for minimum environment and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"Class for a reinforcement learning environment\"\"\"\n",
    "    \n",
    "    def __init__(self, nstate=3, naction=2):\n",
    "        \"\"\"Create a new environment\"\"\"\n",
    "        self.Ns = nstate   # number of states\n",
    "        self.Na = naction  # number of actions\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"start an episode\"\"\"\n",
    "        # randomly pick a state\n",
    "        self.state = np.random.randint(self.Ns)\n",
    "        return(self.state)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"step by an action\"\"\"\n",
    "        # random reward\n",
    "        self.reward = np.random.random()  # between 0 and 1\n",
    "        # shift up/down and rotate in [0,Ns)\n",
    "        self.state = (self.state+(-1 if action==0 else 1))%self.Ns\n",
    "        return(self.reward, self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Class for a reinforcement learning agent\"\"\"\n",
    "    \n",
    "    def __init__(self, nstate, naction):\n",
    "        \"\"\"Create a new agent\"\"\"\n",
    "        self.Ns = nstate   # number of states\n",
    "        self.Na = naction  # number of actions\n",
    "        \n",
    "    def start(self, state):\n",
    "        \"\"\"first action, without reward feedback\"\"\"\n",
    "        # randomly pick an action\n",
    "        self.action = np.random.randint(self.Na)\n",
    "        return(self.action)\n",
    "    \n",
    "    def step(self, reward, state):\n",
    "        \"\"\"learn by reward and take an action\"\"\"\n",
    "        # do nothing for reward\n",
    "        # randomly pick an action\n",
    "        self.action = np.random.randint(self.Na)\n",
    "        return(self.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RL:\n",
    "    \"\"\"Reinforcement learning by interacton of Environment and Agent\"\"\"\n",
    "\n",
    "    def __init__(self, environment, agent, nstate, naction):\n",
    "        \"\"\"Create the environment and the agent\"\"\"\n",
    "        self.env = environment(nstate, naction)\n",
    "        self.agent = agent(nstate, naction)\n",
    "    \n",
    "    def episode(self, tmax=50):\n",
    "        \"\"\"One episode\"\"\"\n",
    "        # First contact\n",
    "        state = self.env.start()\n",
    "        action = self.agent.start(state)\n",
    "        # Table of t, r, s, a\n",
    "        Trsa = np.zeros((tmax+1,4))\n",
    "        Trsa[0,:] = [0, 0, state, action]\n",
    "        # Repeat interactoin\n",
    "        for t in range(1, tmax+1):\n",
    "            reward, state = self.env.step(action)\n",
    "            action = self.agent.step(reward, state)\n",
    "            Trsa[t,:] = [t, reward, state, action]\n",
    "        return(Trsa)\n",
    "    \n",
    "    def run(self, nrun=10, tmax=50):\n",
    "        \"\"\"Multiple runs of episodes\"\"\"\n",
    "        Return = np.zeros(nrun)\n",
    "        for n in range(nrun):\n",
    "            r = self.episode(tmax)[:,1]  # reward sequence\n",
    "            Return[n] = sum(r)\n",
    "        return(Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q learning of Pain-Gain task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PainGain(Environment):\n",
    "    \"\"\"Pain-Gain environment \"\"\"\n",
    "    \n",
    "    def __init__(self, nstate=4, naction=2, gain=6):\n",
    "        self.Ns = nstate   # number of states\n",
    "        self.Na = naction  # number of actions\n",
    "        # setup the reward function as an array\n",
    "        self.R = np.ones((self.Ns, self.Na))\n",
    "        self.R[0,1] = -1   # small pains for action 1\n",
    "        self.R[0,0] = -gain  # large pain\n",
    "        self.R[-1,1] = gain  # large gain\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"step by an action\"\"\"\n",
    "        self.reward = self.R[self.state, action]  # reward\n",
    "        self.state = (self.state + 2*action-1)%self.Ns  # move left or right\n",
    "        return(self.reward, self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QL(Agent):\n",
    "    \"\"\"Class for a Q-learning agent\"\"\"\n",
    "    \n",
    "    def __init__(self, nstate, naction):\n",
    "        self.Ns = nstate   # number of states\n",
    "        self.Na = naction  # number of actions\n",
    "        # allocate Q table\n",
    "        self.Q = np.zeros((nstate, naction))\n",
    "        # default parameters\n",
    "        self.alpha = 0.1  # learning rate\n",
    "        self.beta = 2.0   # inverse temperature\n",
    "        self.gamma = 0.9  # discount factor\n",
    "    \n",
    "    def boltzmann(self, q):\n",
    "        \"\"\"Boltzmann selection\"\"\"\n",
    "        p = np.exp( self.beta*q)   # unnormalized probability\n",
    "        p = p/sum(p)    # probability\n",
    "        # take the index of a segment in [0,1]\n",
    "        return(np.searchsorted( np.cumsum(p), np.random.random()))\n",
    "\n",
    "    def start(self, state):\n",
    "        \"\"\"first action, without reward feedback\"\"\"\n",
    "        # Boltzmann action selection\n",
    "        self.action = self.boltzmann( self.Q[state,:])\n",
    "        # remember the state\n",
    "        self.state = state\n",
    "        return(self.action)\n",
    "    \n",
    "    def step(self, reward, state):\n",
    "        \"\"\"learn by reward and take an action\"\"\"\n",
    "        # TD error: self.state/action retains the previous ones\n",
    "        delta = reward + self.gamma*max(self.Q[state,:]) - self.Q[self.state,self.action]\n",
    "        # Update the value for previous state and action\n",
    "        self.Q[self.state,self.action] += self.alpha*delta\n",
    "        # Boltzmann action selection\n",
    "        self.action = self.boltzmann( self.Q[state,:])\n",
    "        # remember the state\n",
    "        self.state = state\n",
    "        return(self.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pain-Gain environment and Q-learning agent\n",
    "pgq = RL(PainGain, QL, 4, 2)\n",
    "trsa = pgq.episode(100)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(trsa[:,1:]);\n",
    "plt.subplot(2,1,2)\n",
    "plt.bar(np.arange(0,pgq.agent.Ns)-0.4, pgq.agent.Q[:,0], 0.4)  # action 0\n",
    "plt.bar(np.arange(0,pgq.agent.Ns), pgq.agent.Q[:,1], 0.4)  # action 0\n",
    "plt.xlabel(\"state\"); plt.ylabel(\"Q\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
